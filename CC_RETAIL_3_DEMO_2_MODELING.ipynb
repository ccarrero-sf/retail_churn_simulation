{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "notebookId": "6tperthwqvmur4thdvbq",
   "authorId": "5744486210470",
   "authorName": "CCARRERO",
   "authorEmail": "carlos.carrero@snowflake.com",
   "sessionId": "b5db18a5-9403-4b9e-8345-d01e372146c6",
   "lastEditTime": 1741723206928
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25e8678f-29ea-480a-bbce-8642b545e827",
   "metadata": {
    "name": "Intro",
    "collapsed": false
   },
   "source": "# Retail Customers Churn Prediction\n\nThis notebook will use a fictition dataset that contains cusomer data, purchases done by the customer and any feedback provided about the service. The goal is to understand customer purchase behavior, analyze the feedback received and automaticaly identify what customers will not be willing to make more purchases in the shop (churn). \n\nThe demo shows how to combine both structured and unstructured data and process it all securely within Snowflake. It will show how to:\n\n- Use Snowflake Analytical transformation capabilities to understand customer behavior\n- Analyze unstructured data (feedback text, commments, emails, etc) to understand customer sentiment\n- Use Snowflake Feature Store to serve features for training, testing and inference\n- Train Machine Learning models and store them in Snowflake Model Registry\n- Serve models for inference from the Model Registry\n- Use Model Monitoring to detect data drift and models performance\n- Lastly, review Lineage to undertand all process end-to-end\n\nIt will show a fictitious scenario where based on a frequency (dayly, weekly, monthly) sales data is ingested, and customer churn will be predicted so actions can be taken.\n\nA Strealit App will show for the last data ingestion, what are the top customers we should engage and review as they have the greatest possibility of now buying again."
  },
  {
   "cell_type": "code",
   "id": "3775908f-ca36-4846-8f38-5adca39217f2",
   "metadata": {
    "language": "python",
    "name": "cell1"
   },
   "source": "# Import python packages\nimport streamlit as st\nimport pandas as pd\n\n# We can also use Snowpark for our analyses!\nfrom snowflake.snowpark.context import get_active_session\nsession = get_active_session()\n",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "63ea7ba4-2e10-47a3-9f98-8990a59bd6bd",
   "metadata": {
    "name": "cell26",
    "collapsed": false
   },
   "source": "This notebook will be using the DEV schema. Separation of diffent environts can be done using Schemas and RBAC to grant access to different roles. It also simpliffies all CI/CD and MLOps steps.\n\nsales_churn_baseline will be the table where features showing custoemr behavior will be created. For a given timestamp, we will have each customer profile of previous sales and feedback provided.\n\nThis will be used to feed th ML models and inference prediction of buying agian."
  },
  {
   "cell_type": "code",
   "id": "c695373e-ac74-4b62-a1f1-08206cbd5c81",
   "metadata": {
    "language": "python",
    "name": "cell3"
   },
   "source": "session.sql('use schema DEV').collect()\nsession.sql('drop table if exists sales_churn_baseline').collect()",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "9c94780d-3801-4150-8ca2-d73cb3abf484",
   "metadata": {
    "name": "cell33",
    "collapsed": false
   },
   "source": "sales is the table with all transactions. Let's see what is the first and last one we have:"
  },
  {
   "cell_type": "code",
   "id": "f93fb534-768c-4807-a9bb-3a014f9b69f2",
   "metadata": {
    "language": "python",
    "name": "cell69"
   },
   "outputs": [],
   "source": "from snowflake.snowpark import functions as F\n\nsales_df = session.table(\"sales\")\n\nfirst_sale_timestamp = sales_df.select(F.min(F.col(\"transaction_date\"))).collect()[0][0]\n\nlast_sale_timestamp = sales_df.select(F.max(F.col(\"transaction_date\"))).collect()[0][0]\n\ndays_between = (last_sale_timestamp - first_sale_timestamp).days\n\nprint(f'First sale: {first_sale_timestamp}')\nprint(f'Last sale: {last_sale_timestamp}')\nprint(f'Days between first and last sale: {days_between}')\n",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "7774a70c-8423-47ed-b1be-5999ecedf02c",
   "metadata": {
    "name": "cell35",
    "collapsed": false
   },
   "source": "### Training and Testing\n\nWe collect training and testing timestamps and we use them to create a customer profile that will be used to train and test the model. Testing data is after training.\n\nFeatures are created usng the Stored Procedure uc01_feature_engineering that has been defined in the previous notebook. It creates the customer profile for a given timestamp. With that customer profile, we will run predict to determine if a customer is going to churn or not.\n\nAfter creating the profile, we will take a look to the next transactions to determine if the cusotmer really churned or not.\n\nLabeling will create the right CHURN label so we can continuosly monitor the model performance\n\n\n"
  },
  {
   "cell_type": "code",
   "id": "caa7ac84-6607-4164-95f3-b52d2c1ea688",
   "metadata": {
    "language": "python",
    "name": "churn_labels"
   },
   "outputs": [],
   "source": "from datetime import datetime, timedelta\nfrom snowflake.snowpark import functions as F\n\ndb = session.get_current_database()\nsc = session.get_current_schema()\nprint (f'database: {db}, schema: {sc}')\n\nchurn_window = 30 ## This is the value we define as churn\n\nwindows_back =  6 # until streams and tasks are implemented I just add all data from now, so we look back\n\ntable_features = 'churn_baseline'\n\nsession.sql(f'drop table if exists {table_features} ').collect()\n\nsales_df = session.table(\"sales\")\n\n#Find the most recent sales transaction to take it as starting point\nlatest_transaction = sales_df.select(F.max(F.col(\"transaction_date\"))).collect()[0][0]\n\n#Take testing data churn_window before last transaction so we can label it correctly later\ntimestamp_testing = latest_transaction - timedelta(days=churn_window + churn_window * windows_back)\n\nprint (f'timestamp for testing: {timestamp_testing}')\n\n# This will create the profile (feature engineering) for each customer at that timestamp\nsession.call('uc01_feature_engineering_sproc', db, sc, timestamp_testing, table_features)\n\n#Now create the baseline data for training (this happens churn_window days before testing)\ntimestamp_training = timestamp_testing - timedelta(days=churn_window)\n\nprint (f'timestamp for training: {timestamp_training}')\n\n#Perform feature engineering for training data\nsession.call('uc01_feature_engineering_sproc', db, sc, timestamp_training, table_features)\n",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c6844c36-e763-4579-ac76-bc8f3bc38f83",
   "metadata": {
    "name": "cell30",
    "collapsed": false
   },
   "source": "### Label Training and Testing dataset\n\nWe have to label the training and testing data using the function defined in the previous notebook. We call the stored procedure uc_01_label_churn_sproc, pass the baseline table we have created (table_features) and create a new table churn_baseline_labeled with the right CHURN label. 30 is the number of days we use for churn definition"
  },
  {
   "cell_type": "code",
   "id": "0bada144-eef7-4f9d-a356-30cef315f47d",
   "metadata": {
    "language": "python",
    "name": "cell24"
   },
   "outputs": [],
   "source": "session.call('uc_01_label_churn_sproc',table_features, 'churn_baseline_labeled', 30 )",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b3d096af-cd00-42e8-95cb-6c3b96fe528c",
   "metadata": {
    "name": "cell36",
    "collapsed": false
   },
   "source": "Let's review how many churns we have in the training and testing dataset."
  },
  {
   "cell_type": "code",
   "id": "53aba203-1c28-4917-b9d8-8d61ed8e2d68",
   "metadata": {
    "language": "sql",
    "name": "cell34"
   },
   "outputs": [],
   "source": "SELECT \n    TIMESTAMP,\n    SUM(CASE WHEN churned = 0 THEN 1 ELSE 0 END) AS not_churned,\n    SUM(CASE WHEN churned = 1 THEN 1 ELSE 0 END) AS churned\nFROM churn_baseline_labeled\nGROUP BY TIMESTAMP\nORDER BY TIMESTAMP;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e360b2db-b473-4c97-b97b-f7398a56453c",
   "metadata": {
    "name": "cell37",
    "collapsed": false
   },
   "source": "One of the key things is understanding customer feedback. As part of the transformation pipeline, we have already used Snowflake Cortex AI to extract sentiment from customer interactions. Let's take a look:"
  },
  {
   "cell_type": "code",
   "id": "41030813-bdc4-4a9b-aa8b-ccdab2d0c721",
   "metadata": {
    "language": "sql",
    "name": "cell39"
   },
   "outputs": [],
   "source": "select * from FEEDBACK_SENTIMENT where customer_id = 'CUST-96';",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "fc47f863-fc7e-45b0-892c-1b2462643016",
   "metadata": {
    "name": "cell5",
    "collapsed": false
   },
   "source": "## Feature Store & Model Registry\n\nHere we create a new Feature Store and Model Registry"
  },
  {
   "cell_type": "code",
   "id": "5a361655-0bea-43c8-aeb3-41a12e7e7281",
   "metadata": {
    "language": "python",
    "name": "create_fs_and_mr"
   },
   "outputs": [],
   "source": "from snowflake.ml.feature_store import (\n    FeatureStore,\n    FeatureView,\n    Entity,\n    CreationMode)\n\n# Snowflake Model Registry\nfrom snowflake.ml.registry import Registry\n\ndb = session.get_current_database()\nsc = session.get_current_schema()\n\nmr_schema = 'MODEL_REGISTRY'\nfs_schema = 'FEATURE_STORE'\nwarehouse = 'COMPUTE_WH'  #modify as needed.This one is standard in quickstarts\n\n#cleanup - When running this notebook, we are starting from scratch for the demo\n\nsession.sql(f''' drop schema if exists {mr_schema}''').collect()\nsession.sql(\"drop schema if exists FEATURE_STORE\").collect()\n\n\n# Create the Model Registry\ntry:\n    cs = session.get_current_schema()\n    session.sql(f''' create schema {mr_schema} ''').collect()\n    mr = Registry(session=session, database_name= db, schema_name=mr_schema)\n    session.sql(f''' use schema {cs}''').collect()\nexcept:\n    print(f\"Model Registry ({mr_schema}) already exists\")   \n    mr = Registry(session=session, database_name= db, schema_name=mr_schema)\nelse:\n    print(f\"Model Registry ({mr_schema}) created\")\n\n\n# Create the Feature Store\ntry:\n    fs = FeatureStore(session=session, database=db, name=fs_schema, \n                          default_warehouse=warehouse, \n                          creation_mode=CreationMode.FAIL_IF_NOT_EXIST)\n    print(f\"Feature Store ({fs_schema}) already exists\") \nexcept:\n    fs = FeatureStore(session=session, database=db, name=fs_schema, \n                          default_warehouse=warehouse, \n                          creation_mode=CreationMode.CREATE_IF_NOT_EXIST)\n    print(f\"Feature Store ({fs_schema}) created\")   \n",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "401a5f83-dcda-49eb-baeb-880c4546383a",
   "metadata": {
    "name": "cell4",
    "collapsed": false
   },
   "source": "#### CUSTOMER Entity\nNow that we have a Feature Store, the first step is to define the Entities we are going to be using. For this demo, all will be around the customer, and CUSTOMER_ID will be used to join customer, sales and feedback data. When selecting features, will be for customer ids. "
  },
  {
   "cell_type": "code",
   "id": "ca75bfb0-b75c-4075-a753-c56c1d0fa601",
   "metadata": {
    "language": "python",
    "name": "cell9"
   },
   "outputs": [],
   "source": "import json\n\nif \"CUSTOMER_ENT\" not in json.loads(fs.list_entities().select(F.to_json(F.array_agg(\"NAME\", True))).collect()[0][0]):\n    customer_entity = Entity(\n        name=\"CUSTOMER_ENT\", \n        join_keys=[\"CUSTOMER_ID\"],\n        desc=\"Primary Key for CUSTOMER\")\n    fs.register_entity(customer_entity)\nelse:\n    customer_entity = fs.get_entity(\"CUSTOMER_ENT\")\n\nfs.list_entities().show()",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "2b33ab07-5776-4485-8664-a10d2908e2e2",
   "metadata": {
    "name": "cell6",
    "collapsed": false
   },
   "source": "#### Feature View\n\nFor the previous Entity we create a Feature View. Feature transformations can happen within the Feature View or externaly. Parameter feature_df points to either a dataframe with transformations, or a SQL. In that case, Feature Store will be in charge of maintaining those features.\n\nAnother option (like here) is that transformations are done outside. That can be the use case for DBT or using Streams and Tasks within Snowflake. This last is what we have in this demo, as we are pointing the Feature View directly to the sales_churn_baseline were we are storing the features for a given customer and timestamp."
  },
  {
   "cell_type": "code",
   "id": "cd3563a4-2024-4551-a859-7af826727f7f",
   "metadata": {
    "language": "python",
    "name": "define_feature_view"
   },
   "outputs": [],
   "source": "session.sql ('use schema dev').collect()\n\nchurn_df = session.table(\"dev.churn_baseline_labeled\")\n\npreprocess_features_desc = {  \n}\n\nppd_fv_name    = \"FV_UC01_PREPROCESS\"\nppd_fv_version = \"V_1\"\n\ntry:\n   # If FeatureView already exists just return the reference to it\n   fv_uc01_preprocess = fs.get_feature_view(name=ppd_fv_name,version=ppd_fv_version)\nexcept:\n   # Create the FeatureView instance\n   fv_uc01_preprocess_instance = FeatureView(\n      name=ppd_fv_name, \n      entities=[customer_entity], \n      feature_df=churn_df,      # <- We can use the snowpark dataframe as-is from our Python\n      timestamp_col=\"TIMESTAMP\",\n      refresh_freq=None,  # The refresh will be external to feature view (maintained in the pipeline)\n      desc=\"Features to support Churn Detection\").attach_feature_desc(preprocess_features_desc)\n\n   # Register the FeatureView instance.  Creates  object in Snowflake\n   fv_uc01_preprocess = fs.register_feature_view(\n      feature_view=fv_uc01_preprocess_instance, \n      version=ppd_fv_version, \n      block=True\n   )\n   print(f\"Feature View : {ppd_fv_name}_{ppd_fv_version} created\")   \nelse:\n   print(f\"Feature View : {ppd_fv_name}_{ppd_fv_version} already created\")\nfinally:\n   fs.list_feature_views().show(5)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "601bd3e5-925c-4734-80f8-6e89d45c7a19",
   "metadata": {
    "language": "python",
    "name": "cell11"
   },
   "outputs": [],
   "source": "fv_uc01_preprocess.feature_df.show(5)\n",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "7d8584d9-29eb-46d8-9c2e-c0e1899799ec",
   "metadata": {
    "name": "cell7",
    "collapsed": false
   },
   "source": "The Feature Store uses an ASOF join in order to quickly extract features. Here we define the spine for the training and testing datasets using their timestamps:"
  },
  {
   "cell_type": "code",
   "id": "af903913-e60d-4bde-9450-4c74804126ab",
   "metadata": {
    "language": "python",
    "name": "training_spine"
   },
   "outputs": [],
   "source": "# Create Spine for training\n# Timestamp for training was set before when we did the labeling.\n\ntraining_spine_sdf =  fv_uc01_preprocess.feature_df.filter(F.col(\"TIMESTAMP\") == F.lit(timestamp_training)) \\\n                                    .group_by('CUSTOMER_ID').agg(F.max('TIMESTAMP').as_('TIMESTAMP'))\n\n\ntraining_spine_sdf.sort('CUSTOMER_ID').show(5)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "0dc8d4c0-2e8e-4944-b7c3-be60c2a23a7b",
   "metadata": {
    "name": "cell8",
    "collapsed": false
   },
   "source": "#### Training & Testing Dataset\n\nThis is extracted from the Feature View using the spine defined for training and testing. As we create this dataset, it will be kept. This is important so we can understand the lineaage from our Moels and we can review the dataset used for one specific model later if needed."
  },
  {
   "cell_type": "code",
   "id": "2b483500-c389-4e34-aef5-3bede3816664",
   "metadata": {
    "language": "python",
    "name": "training_dataset_def"
   },
   "outputs": [],
   "source": "# Generate_Dataset\ntraining_dataset = fs.generate_dataset( name = 'UC01_TRAINING',\n                                        spine_df = training_spine_sdf, features = [fv_uc01_preprocess], \n                                        spine_timestamp_col = 'TIMESTAMP'\n                                        )                                     \n# Create a snowpark dataframe reference from the Dataset\ntraining_dataset_sdf = training_dataset.read.to_snowpark_dataframe()\n# Display some sample data\ntraining_dataset_sdf.sort('CUSTOMER_ID').show(5)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "da2f1351-0206-4565-b35b-4d0850517925",
   "metadata": {
    "language": "python",
    "name": "cell38"
   },
   "outputs": [],
   "source": "# Create Spine for training this happens after the training one\ntesting_spine_sdf =  fv_uc01_preprocess.feature_df.filter(F.col(\"TIMESTAMP\") == F.lit(timestamp_testing)) \\\n                                    .group_by('CUSTOMER_ID').agg(F.max('TIMESTAMP').as_('TIMESTAMP'))\n\n\ntesting_spine_sdf.sort('CUSTOMER_ID').show(5)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cd28722a-3eef-46a0-9db1-9220426ec5a3",
   "metadata": {
    "language": "python",
    "name": "cell28"
   },
   "outputs": [],
   "source": "# Generate_Dataset for Testing\ntesting_dataset = fs.generate_dataset( name = 'UC01_TESTING',\n                                        spine_df = testing_spine_sdf, features = [fv_uc01_preprocess], \n                                        spine_timestamp_col = 'TIMESTAMP'\n                                        )                                     \n# Create a snowpark dataframe reference from the Dataset\ntesting_dataset_sdf = testing_dataset.read.to_snowpark_dataframe()\n# Display some sample data\ntesting_dataset_sdf.sort('CUSTOMER_ID').show(5)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a038da5d-0ddb-4f86-b390-74c42540db8f",
   "metadata": {
    "language": "python",
    "name": "cell25"
   },
   "outputs": [],
   "source": "session.sql('use schema DEV').collect()",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "caa61676-aa40-4baa-b279-0f8e2f07ef84",
   "metadata": {
    "name": "cell27",
    "collapsed": false
   },
   "source": "## Training the Model\n\nThis defines our function for Training. Using snowflake.ml.modeling for preprocesing and trainign brings the advantage of using Snowflake compute resources. Data is not leaving Snowflake platform and take advantage of Snowflake scalability.\n\nA pipeline is defined that performs some transformations and train the model. Using the pipeline brings the advantage of not having to tranform the features for inference, as that will be done automaticaly.\n\nSnowflake compute is also used to get metrics from the model predictions"
  },
  {
   "cell_type": "code",
   "id": "b97baab3-e9fe-4190-8b3d-5a70cb82b97b",
   "metadata": {
    "language": "python",
    "name": "train_function"
   },
   "outputs": [],
   "source": "from snowflake.ml.modeling.preprocessing import StandardScaler as sml_StandardScaler\nfrom snowflake.ml.modeling.preprocessing import OrdinalEncoder as sml_OrdinalEncoder\nfrom snowflake.ml.modeling.pipeline import Pipeline as sml_Pipeline\nfrom snowflake.ml.modeling.ensemble import RandomForestClassifier\nfrom snowflake.ml.modeling import metrics as snowml_metrics\nfrom snowflake.ml.modeling.xgboost import XGBClassifier\n\n\ndef uc01_train(feature_df):\n\n    train_df, testing_df = feature_df.random_split(weights=[0.8, 0.2], seed=111)\n    \n    oe_input_cols = ['GENDER', 'LOCATION', 'CUSTOMER_SEGMENT']\n    oe_output_cols = ['GENDER_OE', 'LOCATION_OE', 'CUSTOMER_SEGMENT_OE']\n\n    ss_input_numerical_cols = [\n        \"AGE\",\n        \"COUNT_SENTIMENT_PAST_7D\", \"COUNT_SENTIMENT_PAST_1MM\", \"COUNT_SENTIMENT_PAST_2MM\", \"COUNT_SENTIMENT_PAST_3MM\",\n        \"AVG_SENTIMENT_PAST_7D\", \"AVG_SENTIMENT_PAST_1MM\", \"AVG_SENTIMENT_PAST_2MM\", \"AVG_SENTIMENT_PAST_3MM\",\n        \"MIN_SENTIMENT_PAST_7D\", \"MIN_SENTIMENT_PAST_1MM\", \"MIN_SENTIMENT_PAST_2MM\", \"MIN_SENTIMENT_PAST_3MM\",\n        \"SUM_TOTAL_AMOUNT_PAST_7D\", \"SUM_TOTAL_AMOUNT_PAST_1MM\", \"SUM_TOTAL_AMOUNT_PAST_2MM\", \"SUM_TOTAL_AMOUNT_PAST_3MM\",\n        \"COUNT_ORDERS_PAST_7D\", \"COUNT_ORDERS_PAST_1MM\", \"COUNT_ORDERS_PAST_2MM\", \"COUNT_ORDERS_PAST_3MM\"\n    ]\n    \n    ss_output_numerical_cols = [col + \"_SS\" for col in ss_input_numerical_cols]\n    \n    label = ['CHURNED']\n    output_label = ['CHURNED_PRED']\n\n    input_columns = oe_input_cols + ss_input_numerical_cols + label\n    output_columnss = oe_output_cols + ss_output_numerical_cols\n    \n    pipeline_purchases = sml_Pipeline(\n        steps=[ (\"OE\",\n                    sml_OrdinalEncoder(\n                        input_cols=oe_input_cols,\n                        output_cols=oe_output_cols)),\n                (\"SS\",\n                    sml_StandardScaler(\n                        input_cols=ss_input_numerical_cols,\n                        output_cols=ss_output_numerical_cols)),\n                (\"XGB\",\n                    XGBClassifier(\n                        input_cols=output_columnss,\n                        label_cols=label,\n                        output_cols=output_label))\n                ])\n \n    pipeline_purchases.fit(train_df.select(input_columns))\n\n    predictions = pipeline_purchases.predict(train_df)\n\n    accuracy_score = snowml_metrics.accuracy_score (df=predictions, \n                                                    y_true_col_names=['CHURNED'],\n                                                    y_pred_col_names=['CHURNED_PRED'])\n                                                        \n    precision_score = snowml_metrics.precision_score (df=predictions, \n                                                    y_true_col_names=['CHURNED'],\n                                                    y_pred_col_names=['CHURNED_PRED'])\n\n    recall_score = snowml_metrics.recall_score (df=predictions, \n                                                    y_true_col_names=['CHURNED'],\n                                                    y_pred_col_names=['CHURNED_PRED'])   \n    \n    f1_score = snowml_metrics.f1_score (df=predictions, \n                                                    y_true_col_names=['CHURNED'],\n                                                    y_pred_col_names=['CHURNED_PRED'])   \n\n\n    predictions = pipeline_purchases.predict(testing_df)\n\n    t_accuracy_score = snowml_metrics.accuracy_score (df=predictions, \n                                                    y_true_col_names=['CHURNED'],\n                                                    y_pred_col_names=['CHURNED_PRED'])\n                                                        \n    t_precision_score = snowml_metrics.precision_score (df=predictions, \n                                                    y_true_col_names=['CHURNED'],\n                                                    y_pred_col_names=['CHURNED_PRED'])\n\n    t_recall_score = snowml_metrics.recall_score (df=predictions, \n                                                    y_true_col_names=['CHURNED'],\n                                                    y_pred_col_names=['CHURNED_PRED'])   \n    \n    t_f1_score = snowml_metrics.f1_score (df=predictions, \n                                                    y_true_col_names=['CHURNED'],\n                                                    y_pred_col_names=['CHURNED_PRED'])   \n\n\n    \n    return {'MODEL': pipeline_purchases,\n            'accuracy_score': accuracy_score,\n            'precision_score': precision_score,\n            'recall_score': recall_score,\n            'f1_score': f1_score ,\n\n            't_accuracy_score': t_accuracy_score,\n            't_precision_score': t_precision_score,\n            't_recall_score': t_recall_score,\n            't_f1_score': t_f1_score \n           }",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "448c8f27-46df-4c1f-bbf3-d7d680366de6",
   "metadata": {
    "language": "python",
    "name": "train",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# Train the model\ntrained_model = uc01_train(training_dataset_sdf)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c401db99-5a5f-41ec-ba68-861a4eaa45cf",
   "metadata": {
    "language": "python",
    "name": "cell16",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# Lets see the results\ntrained_model",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "43633587-bd07-4e51-a3dc-473dc4c4aa78",
   "metadata": {
    "name": "cell31",
    "collapsed": false
   },
   "source": "We had created a dataset for the testing data. Remember that testing data are transactions that did happen after the training"
  },
  {
   "cell_type": "code",
   "id": "7d6996ca-56e9-4353-9417-21fd28704970",
   "metadata": {
    "language": "python",
    "name": "predict_testing"
   },
   "outputs": [],
   "source": "model= trained_model['MODEL']\n\npredictions = model.predict(testing_dataset_sdf)\n\ntesting_accuracy_score = snowml_metrics.accuracy_score (df=predictions, \n                                                y_true_col_names=['CHURNED'],\n                                                y_pred_col_names=['CHURNED_PRED'])\n                                                    \ntesting_precision_score = snowml_metrics.precision_score (df=predictions, \n                                                y_true_col_names=['CHURNED'],\n                                                y_pred_col_names=['CHURNED_PRED'])\n\ntesting_recall_score = snowml_metrics.recall_score (df=predictions, \n                                                y_true_col_names=['CHURNED'],\n                                                y_pred_col_names=['CHURNED_PRED'])   \n\ntesting_f1_score = snowml_metrics.f1_score (df=predictions, \n                                                y_true_col_names=['CHURNED'],\n                                                y_pred_col_names=['CHURNED_PRED'])   \n\nprint (f'testing_accuracy_score = {testing_accuracy_score}')\nprint (f'testing_precision_score = {testing_precision_score}')\nprint (f'testing_recall_score = {testing_recall_score}')\nprint (f'testing_f1_score = {testing_f1_score}')\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "921fa5a7-09ea-4608-9779-b890d90a1ae4",
   "metadata": {
    "language": "python",
    "name": "cell32"
   },
   "outputs": [],
   "source": "predictions.limit(5)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e30cc57b-6310-481a-8565-e0c8b973cbb4",
   "metadata": {
    "name": "cell41",
    "collapsed": false
   },
   "source": "### Login the Model\n\nWe had created the model registry before. Now se use it to log the model we have just trained. We also store the metrics we have calculated for the model.\n\nThe model name and version are unique"
  },
  {
   "cell_type": "code",
   "id": "b4dd64e4-06f7-481d-89ea-49538d9ce01c",
   "metadata": {
    "language": "python",
    "name": "register_model"
   },
   "outputs": [],
   "source": "from snowflake.ml.model import type_hints\n\nmodel_logged = mr.log_model(model= trained_model['MODEL'],\n                model_name= \"ChurnDetector\",\n                version_name= \"v0\",\n                #conda_dependencies=[\"snowflake-ml-python\"],\n                sample_input_data = training_dataset_sdf.limit(100),\n                #options={\"relax_version\": False, \"enable_explainability\": True},\n                task=type_hints.Task.TABULAR_BINARY_CLASSIFICATION,\n                comment=\"Model to detect what customers will not buy again\"\n                )\n\nmodel_logged.set_metric(metric_name=\"accuracy_score\", value=trained_model['accuracy_score'])\nmodel_logged.set_metric(metric_name=\"precision_score\", value=trained_model['precision_score'])\nmodel_logged.set_metric(metric_name=\"recall_score\", value=trained_model['recall_score'])\nmodel_logged.set_metric(metric_name=\"f1_score\", value=trained_model['f1_score'])\n\nmodel_logged.set_metric(metric_name=\"testing_accuracy_score\", value=testing_accuracy_score)\nmodel_logged.set_metric(metric_name=\"testing_precision_score\", value=testing_precision_score)\nmodel_logged.set_metric(metric_name=\"testing_recall_score\", value=testing_recall_score)\nmodel_logged.set_metric(metric_name=\"testing_f1_score\", value=testing_f1_score)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "393d0679-51c9-4500-9dce-cc500cadbb24",
   "metadata": {
    "name": "cell66",
    "collapsed": false
   },
   "source": "### Inference\n\nWe define two functions to provide inference for a model and dataframe. We use predict to calculate performance metrics and also predict_proba that will be used by the Streamlit App to detect customer with highest probability of not buying again.\n\nData is appended to a table and we filter only relevant columns"
  },
  {
   "cell_type": "code",
   "id": "563f6b78-ed39-4990-95c7-29798700a7db",
   "metadata": {
    "language": "python",
    "name": "cell53"
   },
   "outputs": [],
   "source": "def inference(model, inference_df, output_table):\n\n    inference_result_sdf = model.run(inference_df, function_name=\"predict\")\n\n    columns = [\n        \"CUSTOMER_ID\", \"AGE\", \"GENDER\", \"LOCATION\", \"CUSTOMER_SEGMENT\", \n        \"LAST_PURCHASE_DATE\", \"DAYS_SINCE_LAST_PURCHASE\", \n        \"COUNT_ORDERS_PAST_1MM\", \"COUNT_ORDERS_PAST_2MM\", \"AVG_SENTIMENT_PAST_1MM\", \"AVG_SENTIMENT_PAST_2MM\", \n        \"COUNT_SENTIMENT_PAST_1MM\", \"COUNT_SENTIMENT_PAST_2MM\",\n        \"TIMESTAMP\", \"CHURNED\", \"CHURNED_PRED\"\n    ]\n        \n    inference_result_sdf_final = inference_result_sdf.select(columns)\n\n    inference_result_sdf_final.write.mode(\"append\").save_as_table(output_table)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a1a5f9e5-8a21-455e-bd17-085d0fa7b529",
   "metadata": {
    "language": "python",
    "name": "cell19"
   },
   "outputs": [],
   "source": "def inference_proba(model, inference_df, output_table):\n\n    inference_result_sdf = model.run(inference_df, function_name=\"predict_proba\")\n\n    \n    columns = [\n        \"CUSTOMER_ID\", \"AGE\", \"GENDER\", \"LOCATION\", \"CUSTOMER_SEGMENT\", \n        \"LAST_PURCHASE_DATE\", \"DAYS_SINCE_LAST_PURCHASE\", \n        \"COUNT_ORDERS_PAST_1MM\", \"COUNT_ORDERS_PAST_2MM\", \"AVG_SENTIMENT_PAST_1MM\", \"AVG_SENTIMENT_PAST_2MM\", \n        \"COUNT_SENTIMENT_PAST_1MM\", \"COUNT_SENTIMENT_PAST_2MM\",\n        \"TIMESTAMP\", \"CHURNED\", \"PREDICT_PROBA_1\"\n    ]\n          \n    inference_result_sdf_final = inference_result_sdf.select(columns)\n\n    inference_result_sdf_final.write.mode(\"append\").save_as_table(output_table)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ead7ab2b-4d31-495c-869f-37d4f51a5709",
   "metadata": {
    "language": "sql",
    "name": "cell18"
   },
   "outputs": [],
   "source": "---These are the tables that will be used for model monitoring:\n--- Cleanup to start fresh\n\ndrop table if exists dev.customer_churn_baseline_predicted;\ndrop table if exists dev.customer_churn_predicted;\ndrop table if exists dev.customer_churn_predicted_proba;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "5270b003-21a9-4132-95dc-707ac2881577",
   "metadata": {
    "name": "cell42",
    "collapsed": false
   },
   "source": "#### Baselines for Model Monitoring\n\ncustomer_churn_baseline_predicted is the table that will be used as baseline for Model Monitoring. This is what we used for training, so the Model Monitor can detect any dift on data an generate alarms or re-training.  It will also get the base performance metrics"
  },
  {
   "cell_type": "code",
   "id": "e048cc18-f4f1-41ca-818b-2cabeb229477",
   "metadata": {
    "language": "python",
    "name": "inference_baselines"
   },
   "outputs": [],
   "source": "model = mr.get_model(\"ChurnDetector\").version(\"v0\")\n\ninference(model, training_dataset_sdf, 'dev.customer_churn_baseline_predicted')\ninference(model, testing_dataset_sdf, 'dev.customer_churn_predicted')",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "f514bbda-39fa-4fe9-adf8-616260febb7b",
   "metadata": {
    "name": "cell2",
    "collapsed": false
   },
   "source": "### Simulate to generate new data on a monthly basis\n\nHere we just simulate every week we generate new features and run inference to detect churn for customers that will not buy again in the near future.\n\nAt this point we just create data but will modify it to use streams and task. Goal is to go step by step and show the demo"
  },
  {
   "cell_type": "code",
   "id": "77de59ee-0f59-4773-ba91-856d559119ac",
   "metadata": {
    "language": "sql",
    "name": "cell55"
   },
   "outputs": [],
   "source": "select count(*) from dev.sales;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ef12ab7c-c8e3-4032-b2d6-d17c468dac87",
   "metadata": {
    "language": "sql",
    "name": "cell22"
   },
   "outputs": [],
   "source": "-- Reminder of the features we have calculated:\nSELECT \n    TIMESTAMP,\n    SUM(CASE WHEN churned = 0 THEN 1 ELSE 0 END) AS not_churned,\n    SUM(CASE WHEN churned = 1 THEN 1 ELSE 0 END) AS churned\nFROM churn_baseline_labeled\nGROUP BY TIMESTAMP\nORDER BY TIMESTAMP;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "0a63765c-4686-489f-9222-7bf19f875e05",
   "metadata": {
    "language": "python",
    "name": "cell50"
   },
   "outputs": [],
   "source": "\nchurn_baseline_labeled_df = session.table('churn_baseline_labeled')\n                      \nlatest_feature_timestamp = churn_baseline_labeled_df.select(F.max(F.col(\"timestamp\"))).collect()[0][0]\n\nprint (f'latest feature timestamp:{latest_feature_timestamp} ')\n\nlatest_sales_transaction = session.table('sales').select(F.max(F.col(\"transaction_date\"))).collect()[0][0]\n\nprint (f'latest transaction in sales table:{latest_sales_transaction} ')\n",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "36b19c7b-8d8b-4e23-b9fe-b5c26f7eda31",
   "metadata": {
    "name": "cell44",
    "collapsed": false
   },
   "source": "Get a pointer to the Feature View to be used"
  },
  {
   "cell_type": "code",
   "id": "85215827-c2bf-4c43-bfcc-ea8f032cfcba",
   "metadata": {
    "language": "python",
    "name": "cell64"
   },
   "outputs": [],
   "source": "ppd_fv_name    = \"FV_UC01_PREPROCESS\"\nppd_fv_version = \"V_1\"\n\ntry:\n   # If FeatureView already exists just return the reference to it\n   fv_uc01_preprocess = fs.get_feature_view(name=ppd_fv_name,version=ppd_fv_version)\nexcept:\n    print ('Check this error as the feature view should be already created')\n",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "be56b0a1-e587-4bcd-9051-9a0454748225",
   "metadata": {
    "name": "cell45",
    "collapsed": false
   },
   "source": "This is a simulation for adding new transactions. These are the  steps:\n\n- Add new transactions\n- Predict on those new transactions\n- Correct the churn label on the previous transactions\n\nThis will be used by the model monitor later\n"
  },
  {
   "cell_type": "code",
   "id": "0d85ef9a-dcc5-498f-be64-67d04a6a0b8a",
   "metadata": {
    "language": "python",
    "name": "simulate_data_and_predictions"
   },
   "outputs": [],
   "source": "session.sql('use schema DEV').collect()\n\ndb = session.get_current_database()\nsc = 'DEV'\n\nprint (f'latest feature timestamp:{latest_feature_timestamp} ')\n\nnew_predict_timestamp = latest_feature_timestamp\n\nchurn_window = 30\n\nfor i in range(5):\n    # This will add 7 more days of sales to the sales table\n   \n    new_predict_timestamp = new_predict_timestamp + timedelta(days=churn_window)\n\n    # Build features\n    print (f'Building features for timestamp {new_predict_timestamp} ')\n    session.call('uc01_feature_engineering_sproc', db, sc, new_predict_timestamp, table_features)\n\n    \n    # Update the table used by the feature store (label for latest will not be correct)\n    print ('Updating known labels ')\n    session.call('uc_01_label_churn_sproc',table_features, 'churn_baseline_labeled', 30 )\n\n    #features_to_inference = session.table(churn_baseline_labeled)\n    \n    # Create a dataset with those features\n\n    inference_spine_sdf =  fv_uc01_preprocess.feature_df.filter(F.col(\"TIMESTAMP\") == F.lit(new_predict_timestamp)) \\\n                                    .group_by('CUSTOMER_ID').agg(F.max('TIMESTAMP').as_('TIMESTAMP'))\n\n    formatted_date = str(new_predict_timestamp).replace(\"-\", \"_\")\n\n    print ('generating dataset for inference')\n    name_dataset = f'UC01_INFERENCE_{i}'\n    inference_dataset = fs.generate_dataset( name = name_dataset,\n                                            version = 'v1',\n                                            spine_df = inference_spine_sdf, features = [fv_uc01_preprocess], \n                                            spine_timestamp_col = 'TIMESTAMP'\n                                            )                                     \n    # Create a snowpark dataframe reference from the Dataset\n    inference_dataset_sdf = inference_dataset.read.to_snowpark_dataframe()\n\n    #Run the inference for that dataset\n    inference(model, inference_dataset_sdf, 'dev.customer_churn_predicted')\n\n    inference_proba(model, inference_dataset_sdf, 'dev.customer_churn_predicted_proba')\n\n\n\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5673cd42-d211-45fd-bbca-724193c11d46",
   "metadata": {
    "language": "sql",
    "name": "cell65",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "-- Check there is only one combination for customer_id and timestamp:\n\nwith t as\n(select customer_id, timestamp,\n    count(*) as num_comb \nfrom dev.customer_churn_predicted \ngroup by customer_id, timestamp)\n\nselect num_comb, count(*) as num from t\ngroup by num_comb;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a7478c1d-6e70-4469-9b71-7e0fb7c1bfea",
   "metadata": {
    "language": "sql",
    "name": "cell51"
   },
   "outputs": [],
   "source": "use schema DEV;\n-- Reminder of the features we have calculated:\nSELECT \n    TIMESTAMP,\n    SUM(CASE WHEN churned = 0 THEN 1 ELSE 0 END) AS not_churned,\n    SUM(CASE WHEN churned = 1 THEN 1 ELSE 0 END) AS churned\nFROM churn_baseline_labeled\nGROUP BY TIMESTAMP\nORDER BY TIMESTAMP;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "79282f59-69b9-4f08-893e-b137d28e9300",
   "metadata": {
    "language": "sql",
    "name": "cell56"
   },
   "outputs": [],
   "source": "select timestamp, sum(churned), sum(CHURNED_PRED) from dev.customer_churn_predicted\ngroup by timestamp\norder by timestamp;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c5ed6173-e33f-46cb-8c4e-3dc2ae20f7fc",
   "metadata": {
    "language": "sql",
    "name": "cell57"
   },
   "outputs": [],
   "source": "select min(timestamp), max(timestamp) from dev.customer_churn_predicted;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "8da9b14c-035a-4a91-a2b5-cc3dd64bc221",
   "metadata": {
    "name": "cell46",
    "collapsed": false
   },
   "source": "### Model Monitor\n\nWith a single SQL (we can also use Python API) define the Model Monitor. We provide the table we had created with predictiosn on the training data and the table where we are storign all new predic tions. Create the Model Monitor for the model we had previously trained and testing. "
  },
  {
   "cell_type": "markdown",
   "id": "98e55a49-96bf-4547-bae5-a96e7305e8c4",
   "metadata": {
    "name": "cell43",
    "collapsed": true
   },
   "source": "def inference(model, inference_df, output_table):\n\n    inference_result_sdf = model.run(inference_df, function_name=\"predict\")\n\n    mms_input_numerical_cols = [\n            \"AGE\",\n            \"COUNT_SENTIMENT_PAST_7D\", \"COUNT_SENTIMENT_PAST_1MM\", \"COUNT_SENTIMENT_PAST_2MM\", \"COUNT_SENTIMENT_PAST_3MM\",\n            \"AVG_SENTIMENT_PAST_7D\", \"AVG_SENTIMENT_PAST_1MM\", \"AVG_SENTIMENT_PAST_2MM\", \"AVG_SENTIMENT_PAST_3MM\",\n            \"MIN_SENTIMENT_PAST_7D\", \"MIN_SENTIMENT_PAST_1MM\", \"MIN_SENTIMENT_PAST_2MM\", \"MIN_SENTIMENT_PAST_3MM\",\n            \"SUM_TOTAL_AMOUNT_PAST_7D\", \"SUM_TOTAL_AMOUNT_PAST_1MM\", \"SUM_TOTAL_AMOUNT_PAST_2MM\", \"SUM_TOTAL_AMOUNT_PAST_3MM\",\n            \"COUNT_ORDERS_PAST_7D\", \"COUNT_ORDERS_PAST_1MM\", \"COUNT_ORDERS_PAST_2MM\", \"COUNT_ORDERS_PAST_3MM\"\n        ]\n        \n    mms_output_numerical_cols = [col + \"_MMS\" for col in mms_input_numerical_cols]\n    \n    oe_output_cols = ['GENDER_OE', 'LOCATION_OE', 'CUSTOMER_SEGMENT_OE']\n    \n    drop_columns = mms_output_numerical_cols + oe_output_cols\n    \n    inference_result_sdf_final = inference_result_sdf\n    \n    for column in drop_columns: \n        inference_result_sdf_final = inference_result_sdf_final.drop(F.col(column))\n\n    inference_result_sdf_final.write.mode(\"append\").save_as_table(output_table)"
  },
  {
   "cell_type": "code",
   "id": "aa602fb1-438b-484c-8238-b38b6241a6ef",
   "metadata": {
    "language": "sql",
    "name": "model_monitor"
   },
   "outputs": [],
   "source": "use schema MODEL_REGISTRY;\n\nCREATE OR REPLACE MODEL MONITOR Monitor_ChurnDetector\nWITH\n    MODEL=ChurnDetector\n    VERSION=v0\n    FUNCTION=predict\n    SOURCE=dev.customer_churn_predicted\n    BASELINE=dev.customer_churn_baseline_predicted\n    TIMESTAMP_COLUMN=TIMESTAMP\n    PREDICTION_CLASS_COLUMNS=(CHURNED_PRED)  \n    ACTUAL_CLASS_COLUMNS=(CHURNED)\n    ID_COLUMNS=(CUSTOMER_ID)\n    WAREHOUSE=COMPUTE_WH\n    REFRESH_INTERVAL='1 min'\n    AGGREGATION_WINDOW='1 day';",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cb2f3544-4fca-44a8-b519-19f559ba7560",
   "metadata": {
    "language": "sql",
    "name": "cell29"
   },
   "outputs": [],
   "source": "use schema MODEL_REGISTRY;\n\nDESC MODEL MONITOR Monitor_ChurnDetector;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7865ceed-eddf-4f35-9c7d-8e1dfb0cc55f",
   "metadata": {
    "language": "sql",
    "name": "cell21"
   },
   "outputs": [],
   "source": "select DAYS_SINCE_LAST_PURCHASE, CHURNED, PREDICT_PROBA_1,  from dev.customer_churn_predicted_proba \norder by PREDICT_PROBA_1 desc\nlimit 50;\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "0d236bcd-93f5-4ebc-b2b4-03532e32cfe9",
   "metadata": {
    "language": "sql",
    "name": "cell10"
   },
   "outputs": [],
   "source": "select * from dev.customer_churn_baseline_predicted limit 2;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f2f2ea8e-5ef5-42ef-b4ba-69849a53ada5",
   "metadata": {
    "language": "sql",
    "name": "cell14",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "select * from dev.customer_churn_predicted limit 2;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "584cdee6-5240-4a85-9d10-2c9b02a76fa7",
   "metadata": {
    "language": "sql",
    "name": "cell15"
   },
   "outputs": [],
   "source": "describe table dev.customer_churn_predicted;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b8443ea6-97c7-4203-9125-cb78f2935cb8",
   "metadata": {
    "language": "python",
    "name": "cell17"
   },
   "outputs": [],
   "source": "#retrain model with more recent data\n\n# Create Spine for training\n# Timestamp for training was set before when we did the labeling.\n\nnew_training_spine_sdf =  fv_uc01_preprocess.feature_df.filter(F.col(\"TIMESTAMP\") == F.lit(timestamp_training)) \\\n                                    .group_by('CUSTOMER_ID').agg(F.max('TIMESTAMP').as_('TIMESTAMP'))\n\n\nnew_training_spine_sdf.sort('CUSTOMER_ID').show(5)\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2dd3d5cd-35bc-4066-8021-2c6f8eb71f6a",
   "metadata": {
    "language": "sql",
    "name": "real_churn_metrics"
   },
   "outputs": [],
   "source": "SELECT \n    TIMESTAMP,\n    SUM(CASE WHEN churned = 0 THEN 1 ELSE 0 END) AS not_churned,\n    SUM(CASE WHEN churned = 1 THEN 1 ELSE 0 END) AS churned\nFROM dev.churn_baseline_labeled\nGROUP BY TIMESTAMP\nORDER BY TIMESTAMP;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "f75a14d0-384d-410d-be51-02ec85f78b8f",
   "metadata": {
    "name": "cell54",
    "collapsed": false
   },
   "source": "### Training a 2nd Model Version"
  },
  {
   "cell_type": "code",
   "id": "050611db-7fcc-45b4-8936-701c79275520",
   "metadata": {
    "language": "python",
    "name": "cell48"
   },
   "outputs": [],
   "source": "churn_baseline_labeled_df = session.table('dev.churn_baseline_labeled')\n                      \ndistinct_timestamps = (\n    churn_baseline_labeled_df.select(F.col(\"timestamp\"))\n    .distinct()\n    .sort(F.col(\"timestamp\"))\n)\n\n# Fetch the second minimum timestamp\nsecond_min_timestamp = distinct_timestamps.limit(2).collect()[1][0]\n\nprint(second_min_timestamp)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2f6c6fd2-455b-4b54-b5f0-ee4932d42fb2",
   "metadata": {
    "language": "python",
    "name": "training_second_model"
   },
   "outputs": [],
   "source": "# Create Spine for training based on a more recent month\n# Timestamp for training was set before when we did the labeling.\n\ntraining_spine_sdf2 =  fv_uc01_preprocess.feature_df.filter(F.col(\"TIMESTAMP\") == F.lit(second_min_timestamp)) \\\n                                    .group_by('CUSTOMER_ID').agg(F.max('TIMESTAMP').as_('TIMESTAMP'))\n\n\n\n# Generate_Dataset\ntraining_dataset = fs.generate_dataset( name = 'UC02_TRAINING',\n                                        spine_df = training_spine_sdf2, features = [fv_uc01_preprocess], \n                                        spine_timestamp_col = 'TIMESTAMP'\n                                        )                                     \n# Create a snowpark dataframe reference from the Dataset\ntraining_dataset_sdf = training_dataset.read.to_snowpark_dataframe()\n# Display some sample data\ntraining_dataset_sdf.sort('CUSTOMER_ID').show(5)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cee457d7-24e5-40aa-8187-19a8cac26d37",
   "metadata": {
    "language": "python",
    "name": "cell47"
   },
   "outputs": [],
   "source": "trained_model = uc01_train(training_dataset_sdf)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "17a3a867-035a-400d-afff-0a0f7ed2ebbc",
   "metadata": {
    "language": "python",
    "name": "cell12"
   },
   "outputs": [],
   "source": "trained_model",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "040b8006-0ae6-490b-b728-4d4afc5299c3",
   "metadata": {
    "language": "python",
    "name": "register_second_model"
   },
   "outputs": [],
   "source": "from snowflake.ml.model import type_hints\n\nmodel_logged = mr.log_model(model= trained_model['MODEL'],\n                model_name= \"ChurnDetector\",\n                version_name= \"v1\",\n                #conda_dependencies=[\"snowflake-ml-python\"],\n                sample_input_data = training_dataset_sdf.limit(100),\n                #options={\"relax_version\": False, \"enable_explainability\": True},\n                task=type_hints.Task.TABULAR_BINARY_CLASSIFICATION,\n                comment=\"Model to detect what customers will not buy again\"\n                )\n\nmodel_logged.set_metric(metric_name=\"accuracy_score\", value=trained_model['accuracy_score'])\nmodel_logged.set_metric(metric_name=\"precision_score\", value=trained_model['precision_score'])\nmodel_logged.set_metric(metric_name=\"recall_score\", value=trained_model['recall_score'])\nmodel_logged.set_metric(metric_name=\"f1_score\", value=trained_model['f1_score'])",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "186e633d-3e2e-42d0-8569-71ed2f3a55d5",
   "metadata": {
    "language": "sql",
    "name": "cell40"
   },
   "outputs": [],
   "source": "---These are the tables that will be used for model monitoring:\n--- Cleanup to start fresh\n\ndrop table if exists dev.customer_churn_predicted2;\ndrop table if exists dev.customer_churn_baseline_predicted2;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "25e4f322-77e5-46fb-9bac-88376f458765",
   "metadata": {
    "language": "python",
    "name": "reinterate_inference"
   },
   "outputs": [],
   "source": "model = mr.get_model(\"ChurnDetector\").version(\"v1\")\n\ninference(model, training_dataset_sdf, 'dev.customer_churn_baseline_predicted2')\n#inference(model, testing_dataset_sdf, 'dev.customer_churn_predicted')",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "da36a074-fdba-4962-837a-9e8ab72d2de5",
   "metadata": {
    "language": "python",
    "name": "cell13"
   },
   "outputs": [],
   "source": "from snowflake.ml import dataset\n\nsession.sql('use schema DEV').collect()\n\ndb = session.get_current_database()\nsc = 'DEV'\n\nprint (f'latest feature timestamp:{latest_feature_timestamp} ')\n\nnew_predict_timestamp = latest_feature_timestamp\n\nchurn_window = 30\n\nfor i in range(5):\n   \n    print ('Read dataset for inference')\n    name_dataset = f'FEATURE_STORE.UC01_INFERENCE_{i}'\n\n    inference_dataset = dataset.load_dataset(session, name_dataset, 'v1')\n    # Create a snowpark dataframe reference from the Dataset\n    inference_dataset_sdf = inference_dataset.read.to_snowpark_dataframe()\n\n    #Run the inference for that dataset\n    inference(model, inference_dataset_sdf, 'dev.customer_churn_predicted2')\n\n    inference_proba(model, inference_dataset_sdf, 'dev.customer_churn_predicted_proba2')\n\n\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7fea7064-8d13-4076-96b1-277018112d39",
   "metadata": {
    "language": "sql",
    "name": "cell49"
   },
   "outputs": [],
   "source": "use schema MODEL_REGISTRY;\n\nCREATE OR REPLACE MODEL MONITOR Monitor_ChurnDetector_v1\nWITH\n    MODEL=ChurnDetector\n    VERSION=v1\n    FUNCTION=predict\n    SOURCE=dev.customer_churn_predicted2\n    BASELINE=dev.customer_churn_baseline_predicted2\n    TIMESTAMP_COLUMN=TIMESTAMP\n    PREDICTION_CLASS_COLUMNS=(CHURNED_PRED)  \n    ACTUAL_CLASS_COLUMNS=(CHURNED)\n    ID_COLUMNS=(CUSTOMER_ID)\n    WAREHOUSE=COMPUTE_WH\n    REFRESH_INTERVAL='1 min'\n    AGGREGATION_WINDOW='1 day';",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "88d8ed18-ea61-4be7-8b41-cdf4a95b2f11",
   "metadata": {
    "name": "cell58",
    "collapsed": false
   },
   "source": "### Training 3rd Model"
  },
  {
   "cell_type": "code",
   "id": "2b67e6d0-2c67-4022-a69b-e93991ab3d6d",
   "metadata": {
    "language": "python",
    "name": "cell23"
   },
   "outputs": [],
   "source": "churn_baseline_labeled_df = session.table('dev.churn_baseline_labeled')\n                      \ndistinct_timestamps = (\n    churn_baseline_labeled_df.select(F.col(\"timestamp\"))\n    .distinct()\n    .sort(F.col(\"timestamp\"))\n)\n\n# Fetch the second minimum timestamp\nthird_min_timestamp = distinct_timestamps.limit(3).collect()[2][0]\n\nprint(second_min_timestamp)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "855922a9-b9fa-495a-bc61-871f86f611fe",
   "metadata": {
    "language": "python",
    "name": "cell52"
   },
   "outputs": [],
   "source": "# Create Spine for training based on a more recent month\n# Timestamp for training was set before when we did the labeling.\n\ntraining_spine_sdf3 =  fv_uc01_preprocess.feature_df.filter(F.col(\"TIMESTAMP\") == F.lit(third_min_timestamp)) \\\n                                    .group_by('CUSTOMER_ID').agg(F.max('TIMESTAMP').as_('TIMESTAMP'))\n\n\n\n# Generate_Dataset\ntraining_dataset = fs.generate_dataset( name = 'UC03_TRAINING',\n                                        spine_df = training_spine_sdf3, features = [fv_uc01_preprocess], \n                                        spine_timestamp_col = 'TIMESTAMP'\n                                        )                                     \n# Create a snowpark dataframe reference from the Dataset\ntraining_dataset_sdf = training_dataset.read.to_snowpark_dataframe()\n# Display some sample data\ntraining_dataset_sdf.sort('CUSTOMER_ID').show(5)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1f05be26-b7b2-4210-93ee-eb17fa675bf8",
   "metadata": {
    "language": "python",
    "name": "cell59"
   },
   "outputs": [],
   "source": "trained_model = uc01_train(training_dataset_sdf)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7dfc00f5-1ce0-4d3a-ae40-5efb2fe55bfb",
   "metadata": {
    "language": "python",
    "name": "cell60"
   },
   "outputs": [],
   "source": "trained_model",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4bc98112-e16f-4d23-acc6-c7e1e79a4181",
   "metadata": {
    "language": "python",
    "name": "cell61"
   },
   "outputs": [],
   "source": "from snowflake.ml.model import type_hints\n\nmodel_logged = mr.log_model(model= trained_model['MODEL'],\n                model_name= \"ChurnDetector\",\n                version_name= \"v2\",\n                #conda_dependencies=[\"snowflake-ml-python\"],\n                sample_input_data = training_dataset_sdf.limit(100),\n                #options={\"relax_version\": False, \"enable_explainability\": True},\n                task=type_hints.Task.TABULAR_BINARY_CLASSIFICATION,\n                comment=\"Model to detect what customers will not buy again\"\n                )\n\nmodel_logged.set_metric(metric_name=\"accuracy_score\", value=trained_model['accuracy_score'])\nmodel_logged.set_metric(metric_name=\"precision_score\", value=trained_model['precision_score'])\nmodel_logged.set_metric(metric_name=\"recall_score\", value=trained_model['recall_score'])\nmodel_logged.set_metric(metric_name=\"f1_score\", value=trained_model['f1_score'])",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "dac6f14d-3807-4ab9-be44-c3f434fb04db",
   "metadata": {
    "language": "sql",
    "name": "cell62"
   },
   "outputs": [],
   "source": "\ndrop table if exists dev.customer_churn_predicted3;\ndrop table if exists dev.customer_churn_baseline_predicted3;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4819fbda-164d-4cc9-ba4e-450d4033ae65",
   "metadata": {
    "language": "python",
    "name": "cell63"
   },
   "outputs": [],
   "source": "model = mr.get_model(\"ChurnDetector\").version(\"v2\")\n\ninference(model, training_dataset_sdf, 'dev.customer_churn_baseline_predicted3')\n#inference(model, testing_dataset_sdf, 'dev.customer_churn_predicted')",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3c4e95ae-6973-43c3-a9f2-ba3ad95ad242",
   "metadata": {
    "language": "python",
    "name": "cell67"
   },
   "outputs": [],
   "source": "from snowflake.ml import dataset\n\nsession.sql('use schema DEV').collect()\n\nfor i in range(1, 5):\n\n    \n    print ('Read dataset for inference')\n    name_dataset = f'FEATURE_STORE.UC01_INFERENCE_{i}'\n\n    inference_dataset = dataset.load_dataset(session, name_dataset, 'v1')\n    # Create a snowpark dataframe reference from the Dataset\n    inference_dataset_sdf = inference_dataset.read.to_snowpark_dataframe()\n\n    #Run the inference for that dataset\n    inference(model, inference_dataset_sdf, 'dev.customer_churn_predicted3')\n\n    inference_proba(model, inference_dataset_sdf, 'dev.customer_churn_predicted_proba3')",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b4aba501-7c15-4f3e-abb6-850f188772b0",
   "metadata": {
    "language": "sql",
    "name": "cell68"
   },
   "outputs": [],
   "source": "use schema MODEL_REGISTRY;\n\nCREATE OR REPLACE MODEL MONITOR Monitor_ChurnDetector_v2\nWITH\n    MODEL=ChurnDetector\n    VERSION=v2\n    FUNCTION=predict\n    SOURCE=dev.customer_churn_predicted3\n    BASELINE=dev.customer_churn_baseline_predicted3\n    TIMESTAMP_COLUMN=TIMESTAMP\n    PREDICTION_CLASS_COLUMNS=(CHURNED_PRED)  \n    ACTUAL_CLASS_COLUMNS=(CHURNED)\n    ID_COLUMNS=(CUSTOMER_ID)\n    WAREHOUSE=COMPUTE_WH\n    REFRESH_INTERVAL='1 min'\n    AGGREGATION_WINDOW='1 day';",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6cc49083-633b-4322-95b9-dbbd87cdc5aa",
   "metadata": {
    "language": "sql",
    "name": "cell70"
   },
   "outputs": [],
   "source": "SELECT \n    TIMESTAMP,\n    SUM(CASE WHEN churned = 0 THEN 1 ELSE 0 END) AS not_churned,\n    SUM(CASE WHEN churned = 1 THEN 1 ELSE 0 END) AS churned\nFROM dev.customer_churn_predicted3\nGROUP BY TIMESTAMP\nORDER BY TIMESTAMP;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "99fdcc3f-263f-48fc-916f-321c03f6194b",
   "metadata": {
    "name": "Production_productions",
    "collapsed": false
   },
   "source": "### Simulate predictions update the default model "
  },
  {
   "cell_type": "code",
   "id": "acd3da09-ca88-4c89-b0b7-cbb3b3219419",
   "metadata": {
    "language": "sql",
    "name": "cell73"
   },
   "outputs": [],
   "source": "drop table if exists dev.customer_churn_prod_predicted;\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a116f0a1-38f8-45bf-8caf-a37b82bde65f",
   "metadata": {
    "language": "python",
    "name": "cell71"
   },
   "outputs": [],
   "source": "model = mr.get_model(\"ChurnDetector\")\nvers = model.show_versions()\n\nnum_model_versions = vers['name'].count()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "791ee540-89b0-4019-9af7-a44520dcc116",
   "metadata": {
    "language": "python",
    "name": "cell72",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "from snowflake.ml import dataset\n\nsession.sql('use schema DEV').collect()\n\nnum_model_versions = 3\n\nfor i in range(5):\n\n    if i < num_model_versions:\n        version=f'V{i}'\n    else:\n        version=f'V{num_model_versions - 1}'\n\n    print (f'Using model version: {version} ')\n\n    model = mr.get_model(\"ChurnDetector\").version(version)\n    \n    print ('Read dataset for inference')\n    name_dataset = f'FEATURE_STORE.UC01_INFERENCE_{i}'\n\n    inference_dataset = dataset.load_dataset(session, name_dataset, 'v1')\n    # Create a snowpark dataframe reference from the Dataset\n    inference_dataset_sdf = inference_dataset.read.to_snowpark_dataframe()\n\n    #Run the inference for that dataset\n    inference(model, inference_dataset_sdf, 'dev.customer_churn_prod_predicted')\n\n    inference_proba(model, inference_dataset_sdf, 'dev.customer_churn_prod_predicted_proba')\n\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ee0cbc4b-d7c2-4811-a0ff-65b11aba372f",
   "metadata": {
    "language": "python",
    "name": "cell75"
   },
   "outputs": [],
   "source": "from snowflake.ml.model import type_hints\n\nmodel_logged = mr.log_model(model= trained_model['MODEL'],\n                model_name= \"ChurnDetector\",\n                version_name= \"v3\",\n                #conda_dependencies=[\"snowflake-ml-python\"],\n                sample_input_data = training_dataset_sdf.limit(100),\n                #options={\"relax_version\": False, \"enable_explainability\": True},\n                task=type_hints.Task.TABULAR_BINARY_CLASSIFICATION,\n                comment=\"Model to detect what customers will not buy again\"\n                )",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "69dc61b6-4e03-4ddd-8a38-5e2b9caf040e",
   "metadata": {
    "language": "sql",
    "name": "cell74"
   },
   "outputs": [],
   "source": "use schema MODEL_REGISTRY;\n\nCREATE OR REPLACE MODEL MONITOR Monitor_ChurnDetector_prod\nWITH\n    MODEL=ChurnDetector\n    VERSION=v3\n    FUNCTION=predict\n    SOURCE=dev.customer_churn_prod_predicted\n    BASELINE=dev.customer_churn_baseline_predicted\n    TIMESTAMP_COLUMN=TIMESTAMP\n    PREDICTION_CLASS_COLUMNS=(CHURNED_PRED)  \n    ACTUAL_CLASS_COLUMNS=(CHURNED)\n    ID_COLUMNS=(CUSTOMER_ID)\n    WAREHOUSE=COMPUTE_WH\n    REFRESH_INTERVAL='1 min'\n    AGGREGATION_WINDOW='1 day';",
   "execution_count": null
  }
 ]
}