{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "notebookId": "mtylw4k3bh4ftcwggtwg",
   "authorId": "5744486210470",
   "authorName": "CCARRERO",
   "authorEmail": "carlos.carrero@snowflake.com",
   "sessionId": "cbc5fa07-7384-4626-bb55-e442893e92cd",
   "lastEditTime": 1741722985020
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "id": "3775908f-ca36-4846-8f38-5adca39217f2",
   "metadata": {
    "language": "python",
    "name": "cell1"
   },
   "source": "# Import python packages\nimport streamlit as st\nimport pandas as pd\n\n# We can also use Snowpark for our analyses!\nfrom snowflake.snowpark.context import get_active_session\nsession = get_active_session()\n",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "8d50cbf4-0c8d-4950-86cb-114990437ac9",
   "metadata": {
    "name": "cell2",
    "collapsed": false
   },
   "source": "DEV, TEST and PROD deployments will be hold in different SCHEMAS"
  },
  {
   "cell_type": "code",
   "id": "c695373e-ac74-4b62-a1f1-08206cbd5c81",
   "metadata": {
    "language": "python",
    "name": "cell3"
   },
   "source": "# Then, we can use the python name to turn cell2 into a Pandas dataframe\nsession.sql('create or replace schema DEV').collect()\nsession.sql('use schema DEV').collect()\n",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "095b1d43-d017-4338-af92-3e84b86e6b44",
   "metadata": {
    "language": "python",
    "name": "cell27"
   },
   "outputs": [],
   "source": "session.sql('use schema DEV').collect()\n",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "7998a3b0-6e33-4aba-a5ba-18a9e6e42718",
   "metadata": {
    "name": "cell5",
    "collapsed": false
   },
   "source": "Here we are using the power of Cortex LLMs to create some feedback comments"
  },
  {
   "cell_type": "code",
   "id": "90121591-b3b2-4e0e-91d9-0d05bb17c298",
   "metadata": {
    "language": "python",
    "name": "Create_feedback_comments"
   },
   "outputs": [],
   "source": "comments_sql= \"\"\"\n        create or replace table comments_temp (id number, comment VARCHAR) as \n        \n        select 1, snowflake.cortex.complete ('mistral-large2', 'write comment complaining about the product SkiBoots123. They were broken after 2 days of usage. You are very dissatified')\n        UNION\n        select 2, snowflake.cortex.complete ('mistral-large2', 'write comment complaining about a defect in a recent purchase indicating you are not satisfied and will not buy in the shop again. Do not indicate any date, product or shop ')\n        UNION\n        select 3, snowflake.cortex.complete ('mistral-large2', 'write comment complaining about a recent shipment where the package was broken. Do not indicate any date, product or shop ')\n        UNION\n        select 4, snowflake.cortex.complete ('mistral-large2', 'write comment where you complain a litte bit about support not calling you back. Do not indicate any date, product or shop ')\n        UNION\n        select 5, snowflake.cortex.complete ('mistral-large2', 'write a neutral coment about a recent call you had with support. Do not indicate any date, product or shop ')\n        UNION\n        select 6, snowflake.cortex.complete ('mistral-large2', 'write a neutral comment about a recent purchase you have done. Do not indicate any date, product or shop ')\n        UNION\n        select 7, snowflake.cortex.complete ('mistral-large2', 'write a comment indicating you are satisfied with a recent purchase. Do not indicate any date, product or shop ')\n        UNION\n        select 8, snowflake.cortex.complete ('mistral-large2', 'write a comment indicating you are satisfied with a recent purchase and you will recommend the shop. Do not indicate any date, product or shop ')\n        UNION\n        select 9, snowflake.cortex.complete ('mistral-large2', 'write a comment indicating you are satisfied with a recent support received by a shop assistant. Do not indicate any date, product or shop ')\n\n        \"\"\"\n\ncomments_tb = session.sql(comments_sql).collect()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "125b718f-ac5d-4a6e-87cf-fab858582ab2",
   "metadata": {
    "language": "sql",
    "name": "cell4"
   },
   "outputs": [],
   "source": "select * from comments_temp;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5adc9c2e-aa17-4bf2-9b84-d4bb89ec1559",
   "metadata": {
    "language": "sql",
    "name": "cell10"
   },
   "outputs": [],
   "source": "drop table if exists customers;\ndrop table if exists sales;\ndrop table if exists feedback_raw;\ndrop table if exists feedback_sentiment;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "68f04740-6f13-49c4-8356-2ddcf5c2dcea",
   "metadata": {
    "name": "cell7",
    "collapsed": false
   },
   "source": "Function to add some fake data. This will generate customers, transactions and feedback that will be used later to understand customer purchase behaviours and detect when a customer is on high probability of not buying again so we can take some actions"
  },
  {
   "cell_type": "code",
   "id": "41456c5e-f126-41c3-b477-2ed64761cab7",
   "metadata": {
    "language": "python",
    "name": "generate_data_function"
   },
   "outputs": [],
   "source": "import pandas as pd\nimport random\nfrom datetime import datetime, timedelta\nfrom snowflake.snowpark import types as T\nfrom snowflake.snowpark import functions as F\n\nnum_customers = 10000\nnum_transactions = num_customers * 15\nnum_feedback_reports = num_customers * 2\n\nlocations = ['Madrid', 'Barcelona', 'Paris', 'London', 'Munich', 'Rome']\npayment_methods = ['Credit Card', 'Paypal', 'Apple Pay']\ncustomer_segments = ['Planitum', 'Gold', 'Silver', 'Standard']\nproduct_segments = ['Fashion', 'Electronics', 'Beauty', 'Groceries', 'Home', 'Toys', 'Books']\n\n\ndef generate_data (first_transaction_date, period):\n\n    customers = []\n    for i in range(num_customers):\n        customer_id = f\"CUST-{i}\"\n        age = random.randint(18, 99)\n        gender = 'Male' if random.choice([True, False]) else 'Female'\n        location = random.choice(locations)\n        signup_date = first_transaction_date - timedelta(days=random.randint(365,900))\n        customer_segment = random.choice(customer_segments)\n        \n        customers.append ([customer_id, age, gender, location, signup_date, customer_segment])\n\n    df_customers = pd.DataFrame(customers, columns= [\n        \"CUSTOMER_ID\", \"AGE\", \"GENDER\", \"LOCATION\", \"SIGNUP_DATE\", \"CUSTOMER_SEGMENT\"\n    ])\n\n    customers = session.create_dataframe(df_customers).drop_duplicates()\n    customers = customers.with_column(\"signup_date\", F.col(\"signup_date\").cast(T.DateType()))\n    customers.write.mode(\"overwrite\").save_as_table(\"CUSTOMERS\")\n    #customers.write.csv('@CSV_STAGE/customers.csv', overwrite=True, single=True)\n\n\n    transactions = []\n    for i in range(num_transactions):\n        transaction_id = f\"TRANS-{i+1}\"\n        customer_id = f\"CUST-{random.randint(1, num_customers)}\"\n        transaction_date = first_transaction_date + timedelta(days=random.randint(0,period))\n        total_amount = round(random.uniform(50, 5000), 2)\n        num_items = random.randint(1, 10)\n        discount_applied = bool(random.randint(0, 1))\n        payment_method = random.choice(payment_methods)\n        \n        transactions.append([transaction_id, customer_id, transaction_date, total_amount, num_items, discount_applied, payment_method])\n\n    # Creating the DataFrame for transactions\n    df_transactions = pd.DataFrame(transactions, columns=[\n        \"TRANSACTION_ID\", \"CUSTOMER_ID\", \"TRANSACTION_DATE\", \"TOTAL_AMOUNT\", \n        \"NUM_ITEMS\", \"DISCOUNT_APPLIED\", \"PAYMENT_METHOD\"\n    ])\n\n    transactions = session.create_dataframe(df_transactions).drop_duplicates()\n    transactions = transactions.with_column(\"transaction_date\", F.col(\"transaction_date\").cast(T.DateType()))\n    transactions.write.mode(\"overwrite\").save_as_table(\"SALES\")\n    #transactions.write.csv('@CSV_STAGE/transactions.csv', overwrite=True, single=True)\n    \n   # Feedback data generation\n\n    comments_df = session.table(\"comments_temp\")\n    \n    feedback_df = session.sql(f\"\"\"\n        SELECT \n        'FB-' || TO_CHAR(SEQ8()) AS feedback_id,\n        'CUST-' || TO_CHAR(UNIFORM(1, 5000, RANDOM())) AS customer_id,\n        DATEADD(DAY, -UNIFORM(0, 365, RANDOM()), CURRENT_DATE) AS chat_date,\n        UNIFORM(1, 9, RANDOM()) AS internal_id\n    FROM TABLE(GENERATOR(ROWCOUNT => {num_feedback_reports}))\n    \"\"\")\n    \n    final_feedback_df = feedback_df.join(comments_df, feedback_df[\"internal_id\"] == comments_df[\"id\"]) \\\n                                    .select(feedback_df[\"feedback_id\"], \n                                            feedback_df[\"customer_id\"], \n                                            feedback_df[\"chat_date\"], \n                                            feedback_df[\"internal_id\"], \n                                            comments_df[\"comment\"])\n    \n    final_feedback_df.write.mode(\"overwrite\").save_as_table(\"FEEDBACK_RAW\")\n    #final_feedback_df.write.csv('@CSV_STAGE/final_feedback_df.csv', overwrite=True, single=True)\n",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "9a7243a8-1c51-4908-be9c-80754636bd7b",
   "metadata": {
    "name": "cell6",
    "collapsed": false
   },
   "source": "To simulate the data feeds, we are going to generate 4 months of data from 6 months since now"
  },
  {
   "cell_type": "code",
   "id": "f0e134c0-e39c-44c2-80eb-86cf752dee60",
   "metadata": {
    "language": "python",
    "name": "generate_base_data",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "\nnum_days_per_batch = 30\n\nfirst_timestamp = datetime.now().date() - timedelta(days=12* num_days_per_batch)\n\ngenerate_data(first_timestamp, num_days_per_batch * 1)  ",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9d740a90-d041-4e19-860a-b7742d5670bc",
   "metadata": {
    "language": "sql",
    "name": "cell16"
   },
   "outputs": [],
   "source": "select min(transaction_date), max(transaction_date) from sales;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ace2bf07-1b2a-4aa1-b5c4-42bb3cc34234",
   "metadata": {
    "name": "cell9",
    "collapsed": false
   },
   "source": "Here we can use the power of LLMs to understand customer feedback and provide a sentiment score that we will be using later to determine probability of churn\n"
  },
  {
   "cell_type": "code",
   "id": "698e4493-abf7-4626-b44a-14b4a79c22da",
   "metadata": {
    "language": "python",
    "name": "cell8"
   },
   "outputs": [],
   "source": "from snowflake.cortex import sentiment\nimport snowflake.snowpark.functions as F\n\n\nfeedback_raw_df = session.table(\"feedback_raw\")\n\nfeedback_sentiment_df = feedback_raw_df.with_columns([\"sentiment\"], [sentiment(F.col(\"comment\"))])\n\nfeedback_sentiment_df.write.mode(\"overwrite\").save_as_table(\"feedback_sentiment\")",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "1ad82286-e1a5-4343-9dfb-b1f4bfc5121a",
   "metadata": {
    "name": "cell11",
    "collapsed": false
   },
   "source": "### Feature Engineering\n\nThis function will create a profile for each customer who has already made a purchase in the shop. For a given timestamp, it will analyze the past transactions and feedback and using Snowflake analytical functions will generate features that will be used to traind and predict models.\n\nWe will store it as a Stored Procedure so it can be called from anywhere"
  },
  {
   "cell_type": "code",
   "id": "17760d31-3103-44b6-a3b2-80789b373eb4",
   "metadata": {
    "language": "python",
    "name": "cell12"
   },
   "outputs": [],
   "source": "session.sql(\"CREATE stage IF NOT EXISTS ML_STAGE\").collect()\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2d5ab091-8201-42ae-b781-00c8e85df8ad",
   "metadata": {
    "language": "sql",
    "name": "cell25"
   },
   "outputs": [],
   "source": "use schema dev;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "014e8fa7-8c11-4a75-b741-50bf914eb234",
   "metadata": {
    "language": "python",
    "name": "features_and_Label"
   },
   "outputs": [],
   "source": "import pandas as pd\nimport random\nfrom datetime import datetime, timedelta\nfrom snowflake.snowpark import types as T\nfrom snowflake.snowpark import functions as F\nfrom snowflake import snowpark\n\ndef uc01_feature_engineering(session: snowpark.Session, db: str, sc:str, cur_date: datetime, table_name: str):\n\n    # In oder to label as churn or not, we have to take a look to the features churn_days before the day\n    # we are doing the calculation. We calculater metrics based on that daytme and if there are no\n    # transactions between the cur_date and that number of churn_days_number we label it as churn\n    # we do this for having the real label. On new data, we just calculate the feature metrics\n    \n    # Load data\n    customers_tbl = '.'.join([db, sc,'CUSTOMERS'])\n    sales_tbl = '.'.join([db, sc,'SALES'])\n    feedback_tbl = '.'.join([db, sc,'FEEDBACK_SENTIMENT'])\n    \n    customers_df = session.table(customers_tbl)\n    sales_df = session.table(sales_tbl)\n    sales_df_last_tran = session.table(sales_tbl)\n\n    feedback_sentiment_df = session.table(feedback_tbl)    \n\n    # we are only doing feature engineering for transactions before cur_date\n    \n    sales_df_last_tran = sales_df_last_tran.filter(F.col(\"transaction_date\") < F.lit(cur_date))\n\n    sales_df = sales_df.filter(F.col(\"transaction_date\") < F.lit(cur_date ))\n        \n    # count only feedback before cur_date\n    \n    feedback_sentiment_df = feedback_sentiment_df.filter(F.col(\"chat_date\") < F.lit(cur_date))\n    \n    sales_agg_df = (\n        sales_df_last_tran.group_by(\"customer_id\")\n        .agg(\n            F.max(\"transaction_date\").alias(\"last_purchase_date\"),\n            F.sum(\"total_amount\").alias(\"total_customer_value\")\n        )\n    )\n    \n    def custom_column_naming(input_col, agg, window):\n        return f\"{agg}_{input_col}_{window.replace('-', 'past_')}\"\n                                                   \n    sales_agg_orders_df = sales_df.analytics.time_series_agg(\n            time_col=\"transaction_date\",\n            aggs={\"total_amount\": [\"SUM\", \"COUNT\"]},\n            windows=[\"-7D\",\"-1MM\", \"-2MM\", \"-3MM\"],\n            sliding_interval=\"1D\",\n            group_by=[\"CUSTOMER_ID\"],\n            col_formatter = custom_column_naming)\n\n    sales_agg_last_purchase = sales_agg_df.join(\n        sales_agg_orders_df,\n        (sales_agg_df.last_purchase_date == sales_agg_orders_df.transaction_date) &\n        (sales_agg_df.CUSTOMER_ID == sales_agg_orders_df.CUSTOMER_ID),\n        \"left\").select(\n            sales_agg_df[\"customer_id\"].alias(\"CUSTOMER_ID\"),\n            sales_agg_df[\"total_customer_value\"],\n            sales_agg_df[\"last_purchase_date\"],\n            sales_agg_orders_df[\"SUM_TOTAL_AMOUNT_PAST_7D\"],\n            sales_agg_orders_df[\"SUM_TOTAL_AMOUNT_PAST_1MM\"],\n            sales_agg_orders_df[\"SUM_TOTAL_AMOUNT_PAST_2MM\"],\n            sales_agg_orders_df[\"SUM_TOTAL_AMOUNT_PAST_3MM\"],\n            sales_agg_orders_df[\"COUNT_TOTAL_AMOUNT_PAST_7D\"],\n            sales_agg_orders_df[\"COUNT_TOTAL_AMOUNT_PAST_1MM\"],\n            sales_agg_orders_df[\"COUNT_TOTAL_AMOUNT_PAST_2MM\"],\n            sales_agg_orders_df[\"COUNT_TOTAL_AMOUNT_PAST_3MM\"]\n        )\n\n    #  feedback data\n\n    latest_feedback_df = (feedback_sentiment_df.group_by(\"customer_id\")\n            .agg(F.max(\"chat_date\").alias(\"chat_date\")))\n    \n    feedback_agg_df = feedback_sentiment_df.analytics.time_series_agg(\n            time_col=\"chat_date\",\n            aggs={\"sentiment\": [\"COUNT\", \"AVG\", \"MIN\"]},\n            windows=[\"-7D\",\"-1MM\", \"-2MM\", \"-3MM\"],\n            sliding_interval=\"1D\",\n            group_by=[\"CUSTOMER_ID\"],\n            col_formatter = custom_column_naming)\n\n    feedback_agg_latest_df = latest_feedback_df.join(\n        feedback_agg_df, \"chat_date\", \"left\").select(\n            latest_feedback_df[\"CUSTOMER_ID\"].alias(\"CUSTOMER_ID\"),\n            feedback_agg_df[\"COUNT_SENTIMENT_PAST_7D\"],\n            feedback_agg_df[\"COUNT_SENTIMENT_PAST_1MM\"],\n            feedback_agg_df[\"COUNT_SENTIMENT_PAST_2MM\"],\n            feedback_agg_df[\"COUNT_SENTIMENT_PAST_3MM\"],\n            feedback_agg_df[\"AVG_SENTIMENT_PAST_7D\"],\n            feedback_agg_df[\"AVG_SENTIMENT_PAST_1MM\"],\n            feedback_agg_df[\"AVG_SENTIMENT_PAST_2MM\"],\n            feedback_agg_df[\"AVG_SENTIMENT_PAST_3MM\"],\n            feedback_agg_df[\"MIN_SENTIMENT_PAST_7D\"],\n            feedback_agg_df[\"MIN_SENTIMENT_PAST_1MM\"],\n            feedback_agg_df[\"MIN_SENTIMENT_PAST_2MM\"],\n            feedback_agg_df[\"MIN_SENTIMENT_PAST_3MM\"],          \n        )\n\n    feedback_agg_latest_df.limit(10)\n    \n    # Join tables\n    features_df = (\n        customers_df.join(sales_agg_last_purchase, \"customer_id\", \"left\")\n        .join(feedback_agg_latest_df, \"customer_id\", \"left\")\n        .select(\n            customers_df[\"customer_id\"],\n            customers_df[\"age\"],\n            customers_df[\"gender\"],\n            customers_df[\"location\"],\n            customers_df[\"customer_segment\"],\n            sales_agg_last_purchase[\"last_purchase_date\"],\n            feedback_agg_latest_df[\"COUNT_SENTIMENT_PAST_7D\"],\n            feedback_agg_latest_df[\"COUNT_SENTIMENT_PAST_1MM\"],\n            feedback_agg_latest_df[\"COUNT_SENTIMENT_PAST_2MM\"],\n            feedback_agg_latest_df[\"COUNT_SENTIMENT_PAST_3MM\"],\n            feedback_agg_latest_df[\"AVG_SENTIMENT_PAST_7D\"],\n            feedback_agg_latest_df[\"AVG_SENTIMENT_PAST_1MM\"],\n            feedback_agg_latest_df[\"AVG_SENTIMENT_PAST_2MM\"],\n            feedback_agg_latest_df[\"AVG_SENTIMENT_PAST_3MM\"],\n            feedback_agg_latest_df[\"MIN_SENTIMENT_PAST_7D\"],\n            feedback_agg_latest_df[\"MIN_SENTIMENT_PAST_1MM\"],\n            feedback_agg_latest_df[\"MIN_SENTIMENT_PAST_2MM\"],\n            feedback_agg_latest_df[\"MIN_SENTIMENT_PAST_3MM\"],\n            sales_agg_last_purchase[\"SUM_TOTAL_AMOUNT_PAST_7D\"],\n            sales_agg_last_purchase[\"SUM_TOTAL_AMOUNT_PAST_1MM\"],\n            sales_agg_last_purchase[\"SUM_TOTAL_AMOUNT_PAST_2MM\"],\n            sales_agg_last_purchase[\"SUM_TOTAL_AMOUNT_PAST_3MM\"],\n            sales_agg_last_purchase[\"COUNT_TOTAL_AMOUNT_PAST_7D\"].alias(\"COUNT_ORDERS_PAST_7D\"),\n            sales_agg_last_purchase[\"COUNT_TOTAL_AMOUNT_PAST_1MM\"].alias(\"COUNT_ORDERS_PAST_1MM\"),\n            sales_agg_last_purchase[\"COUNT_TOTAL_AMOUNT_PAST_2MM\"].alias(\"COUNT_ORDERS_PAST_2MM\"),\n            sales_agg_last_purchase[\"COUNT_TOTAL_AMOUNT_PAST_3MM\"].alias(\"COUNT_ORDERS_PAST_3MM\"),\n            F.datediff(\"day\", sales_agg_df[\"last_purchase_date\"], F.lit(cur_date)).alias(\"DAYS_SINCE_LAST_PURCHASE\"),\n            F.lit(cur_date).alias(\"TIMESTAMP\")\n        ).filter(sales_agg_df[\"last_purchase_date\"].isNotNull()  # Avoid customers never purchased\n        ).dropDuplicates([\"customer_id\", \"TIMESTAMP\"])  # Ensure one combination of customer_id and TIMESTAMP\n\n    )\n    \n    # Fill with 0 those where we have no data (so neutral feedback and zero iterations and amount)\n    columns_to_fill = [\n        \"COUNT_SENTIMENT_PAST_7D\", \"COUNT_SENTIMENT_PAST_1MM\", \"COUNT_SENTIMENT_PAST_2MM\", \"COUNT_SENTIMENT_PAST_3MM\",\n        \"AVG_SENTIMENT_PAST_7D\", \"AVG_SENTIMENT_PAST_1MM\", \"AVG_SENTIMENT_PAST_2MM\", \"AVG_SENTIMENT_PAST_3MM\",\n        \"MIN_SENTIMENT_PAST_7D\", \"MIN_SENTIMENT_PAST_1MM\", \"MIN_SENTIMENT_PAST_2MM\", \"MIN_SENTIMENT_PAST_3MM\",\n        \"SUM_TOTAL_AMOUNT_PAST_7D\", \"SUM_TOTAL_AMOUNT_PAST_1MM\", \"SUM_TOTAL_AMOUNT_PAST_2MM\", \"SUM_TOTAL_AMOUNT_PAST_3MM\",\n        \"COUNT_ORDERS_PAST_7D\", \"COUNT_ORDERS_PAST_1MM\", \"COUNT_ORDERS_PAST_2MM\", \"COUNT_ORDERS_PAST_3MM\"\n    ]\n    \n    for column in columns_to_fill:\n        features_df = features_df.fillna({column: 0})\n    \n    # Write to Snowflake Table\n    features_df.write.mode(\"append\").save_as_table(table_name)\n\n    print (f'Created table {table_name}')\n\nsession.sproc.register(\n    func=uc01_feature_engineering,\n    name=\"uc01_feature_engineering_sproc\",\n    replace=True,\n    is_permanent=True,\n    stage_location=\"@ML_STAGE\",\n    packages=['snowflake-snowpark-python', 'snowflake-ml-python'],\n    return_type=T.StringType()\n)\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3848256a-33fd-4d13-9364-ffa370d49616",
   "metadata": {
    "language": "python",
    "name": "cell23"
   },
   "outputs": [],
   "source": "sales_df = session.table(\"sales\")\n\nfirst_sale_timestamp = sales_df.select(F.min(F.col(\"transaction_date\"))).collect()[0][0]\n\nlast_sale_timestamp = sales_df.select(F.max(F.col(\"transaction_date\"))).collect()[0][0]\n\nprint (f'First sale:{first_sale_timestamp}')\nprint (f'Last sale: {last_sale_timestamp}')",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "0588b582-481f-4a3c-b638-0b2c9828deb8",
   "metadata": {
    "language": "python",
    "name": "cell21"
   },
   "outputs": [],
   "source": "",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "1358bbd2-c5f6-4bfa-9de5-edd55e2f684e",
   "metadata": {
    "name": "cell17",
    "collapsed": false
   },
   "source": "### Generate monthly purchase based on customer behaviors"
  },
  {
   "cell_type": "code",
   "id": "ffec4d3b-6e75-443d-8c89-963a968c849a",
   "metadata": {
    "language": "python",
    "name": "cell24"
   },
   "outputs": [],
   "source": "session.sql('use schema DEV;').collect()\nprint (session.get_current_schema())",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "220ad6ff-21e2-4908-8156-b9d5ddf8bd6f",
   "metadata": {
    "language": "python",
    "name": "add_more_sales_with_churn"
   },
   "outputs": [],
   "source": "import pandas as pd\nimport random\nfrom datetime import datetime, timedelta\nfrom snowflake.snowpark import types as T\nfrom snowflake.snowpark import functions as F\nfrom snowflake import snowpark\n\ndef add_new_top_sales (session: snowpark.Session, churn_base_table: str, \n                       new_sales_table:str , days_window: int):\n\n    churn_df = session.table(churn_base_table)\n    num_cust = churn_df.count()\n        \n    latest_timestamp = churn_df.select(F.max(F.col(\"TIMESTAMP\"))).collect()[0][0]\n    \n    churn_df = churn_df.filter(F.col(\"TIMESTAMP\") == F.lit(latest_timestamp))\n    \n    top_sentiment_customers_df = (\n            churn_df.order_by(F.col(\"AVG_SENTIMENT_PAST_1MM\").desc())).limit(round(num_cust*0.1))\n\n    top_sentiment_customers_df = (\n        churn_df\n        .filter(F.col(\"AVG_SENTIMENT_PAST_1MM\") > 0)  # Filter out to positive sentiment\n        .orderBy(F.col(\"AVG_SENTIMENT_PAST_1MM\").desc())  # Order by descending sentiment\n        .limit(round(num_cust * 0.1))  # Select top 10% customers\n    )\n    \n    top_sentiment_customers_df = top_sentiment_customers_df.select(F.col(\"customer_id\")).collect()\n    \n    top_orders_customers_df = churn_df.order_by(F.col(\"COUNT_ORDERS_PAST_1MM\").desc()).limit(round(num_cust*0.1))\n    top_orders_customers_df = top_orders_customers_df.select(F.col(\"customer_id\")).collect()\n\n    #get a ramdom sample of customers to buy again\n    sample_customers_buy_again = churn_df.select(F.col(\"customer_id\"))\\\n            .sample(frac=0.1).collect()\n    \n    last_sales_timestamp = session.table(\"sales\").select(F.max(F.col(\"transaction_date\"))).collect()[0][0]\n    \n    def add_transactions (df, last_sales_timestamp, days_window):\n    \n        transactions = []\n        \n        for customer in df:\n\n            for i in range(random.randint(1,5)): #between 1 and 10 transactions in the period\n                customer_id = customer[\"CUSTOMER_ID\"]\n                    \n                payment_methods = ['Credit Card', 'Paypal', 'Apple Pay']\n            \n                transaction_id = f\"TRANS-N\"\n                customer_id = customer_id\n                \n                #if round(random.randint(1,10)) >= 6: # 60% buy in the entire period\n                days=random.randint(1,days_window)\n                #else: # the rest buy in the first quarter\n                #    days=random.randint(1,round(days_window/4)+1)\n\n                transaction_date = last_sales_timestamp + timedelta(days=random.randint(1,days))\n                \n                total_amount = round(random.uniform(50, 5000), 2)\n                num_items = random.randint(1, 5)\n                discount_applied = bool(random.randint(0, 1))\n                payment_method = random.choice(payment_methods)\n                    \n                transactions.append([transaction_id, customer_id, transaction_date, total_amount, num_items, discount_applied, payment_method])\n        \n        # Creating the DataFrame for transactions\n        df_transactions = pd.DataFrame(transactions, columns=[\n            \"TRANSACTION_ID\", \"CUSTOMER_ID\", \"TRANSACTION_DATE\", \"TOTAL_AMOUNT\", \n            \"NUM_ITEMS\", \"DISCOUNT_APPLIED\", \"PAYMENT_METHOD\"\n        ])\n        \n        df_transactions[\"TRANSACTION_DATE\"] = pd.to_datetime(df_transactions[\"TRANSACTION_DATE\"], errors=\"coerce\")\n    \n        transactions = session.create_dataframe(df_transactions).drop_duplicates()\n        transactions = transactions.with_column(\"TRANSACTION_DATE\", F.to_date(F.col(\"TRANSACTION_DATE\")))\n\n       # transactions = transactions.with_column(\"transaction_date\", F.col(\"transaction_date\").cast(T.DateType()))\n        transactions.write.mode(\"append\").save_as_table(new_sales_table)\n     \n        num_tran = transactions.count()\n        print (f'added {num_tran} transactions')\n\n    print (\"writing top sentiment custoemrs\")\n    add_transactions(top_sentiment_customers_df, last_sales_timestamp, days_window)\n\n    print (\"writing top orders custoemrs\")\n    add_transactions(top_orders_customers_df, last_sales_timestamp, days_window)\n\n    print (\"writing sample from all customers\")\n    add_transactions(sample_customers_buy_again, last_sales_timestamp, days_window)\n    \n    #angry customers\n    angry_customers_df = (\n            churn_df.order_by(F.col(\"AVG_SENTIMENT_PAST_1MM\").asc())).limit(round(num_cust*0.05))\n\n    angry_sentiment_customers_df = angry_customers_df.select(F.col(\"customer_id\")).collect()\n\n    #even some angry custoemrs buy again!\n    angry_sample_buy_df = angry_customers_df\\\n            .select(F.col(\"customer_id\"))\\\n            .sample(frac=0.1).collect()\n \n    print (\"Adding angry customers\")\n    add_transactions(angry_sample_buy_df, last_sales_timestamp, days_window)\n \n    #add sentiment but no transactions (customer keep complaining)\n    feedback = []\n\n    for customer in angry_sentiment_customers_df:\n        feedback_id = 'FEEDBACK-N'\n        customer_id = customer[\"CUSTOMER_ID\"]\n        customer_id = customer_id\n        chat_date = last_sales_timestamp + timedelta(days=random.randint(1,days_window))\n        internal_id = random.randint(1, 3) #from angry to neutral\n        feedback.append([feedback_id, customer_id, chat_date, internal_id])\n\n    df_feedback = pd.DataFrame(feedback, columns=[\n            \"FEEDBACK_ID\", \"CUSTOMER_ID\", \"CHAT_DATE\",\n            \"INTERNAL_ID\"\n    ])\n\n    print (\"writing feedback\")\n    feedback = session.create_dataframe(df_feedback).drop_duplicates()\n    feedback = feedback.with_column(\"chat_date\", F.col(\"chat_date\").cast(T.DateType()))\n    feedback = feedback.with_column(\"customer_id\", F.col(\"customer_id\").cast(T.StringType()))\n\n    feedback.write.mode(\"overwrite\").save_as_table(\"temp_feedback\")\n\n    feedback_df = session.table(\"temp_feedback\")\n    \n    comments_df = session.table(\"comments_temp\")\n    \n    temp_feedback_df = feedback_df.join(comments_df, feedback_df[\"internal_id\"] == comments_df[\"id\"]) \\\n                                    .select(feedback_df[\"feedback_id\"], \n                                            feedback_df[\"customer_id\"], \n                                            feedback_df[\"chat_date\"], \n                                            feedback_df[\"internal_id\"], \n                                            comments_df[\"comment\"])\n  \n    feedback_sentiment_df = temp_feedback_df.with_columns([\"sentiment\"], [sentiment(F.col(\"comment\"))])\n  \n    feedback_sentiment_df.write.mode(\"append\").save_as_table('feedback_sentiment')\n\n    return \"Transactions added\"\n\nsession.sproc.register(\n    func=add_new_top_sales,\n    name=\"add_new_top_sales_sproc\",\n    replace=True,\n    is_permanent=True,\n    stage_location=\"@ML_STAGE\",\n    packages=['snowflake-snowpark-python', 'snowflake-ml-python'],\n    return_type=T.StringType()\n)\n\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "72aadc0a-0108-4a4d-b5f5-f9139524d251",
   "metadata": {
    "language": "python",
    "name": "cell26"
   },
   "outputs": [],
   "source": "",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d62d163f-e002-4b87-a16b-f9825d0f0bfb",
   "metadata": {
    "language": "python",
    "name": "cell15"
   },
   "outputs": [],
   "source": "",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "31cca694-ad80-4fa2-9aca-9429f8bfcf68",
   "metadata": {
    "language": "python",
    "name": "generate_more_data"
   },
   "outputs": [],
   "source": "# Build features for the timestamp of the last sales. This will create the first customer \n# behavior profile in the training baseline baseline_features_builing_dataset\n# With theat profile, we call the function to add more sales based on that customer profile\n\ndb = session.get_current_database()\nsc = 'DEV'\ntable_features = 'baseline_features_builing_dataset'\n\nsession.sql(f'use schema {sc}').collect()\nsession.sql(f'drop table if exists {table_features}').collect()\n\nsales_df = session.table(\"sales\")\nn_transactions = sales_df.count()\nprint (n_transactions)\n\nfeedback_sentiment_df = session.table(\"feedback_sentiment\")\nnum_reviews = feedback_sentiment_df.count()\nprint (f'num reviews: {num_reviews}')\n\nfor i in range(12):\n\n    # Customers profiles fora given timestmp (last sales date)\n    sales_df = session.table(\"sales\")\n\n    last_sale_timestamp = sales_df.select(F.max(F.col(\"transaction_date\"))).collect()[0][0]\n\n    print (f'Building features for timestamp: {last_sale_timestamp}')\n    uc01_feature_engineering (session, db, sc, last_sale_timestamp, table_features)\n\n    #add 30 more days of sales to thee sales table, based on the last\n    #profile of custoemr_churn_testing\n    print (f'adding more sales')\n    add_new_top_sales (session, table_features, 'sales',30)\n\n    n_transactions = sales_df.count()\n    print (n_transactions)\n    ",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e8d4123c-16ab-4669-b6ee-fc9e39def22a",
   "metadata": {
    "language": "sql",
    "name": "cell20"
   },
   "outputs": [],
   "source": "select timestamp,  count(*) from baseline_features_builing_dataset\ngroup by timestamp\norder by timestamp desc;\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "48925cc5-09fe-4290-ad94-8b30025ab998",
   "metadata": {
    "language": "sql",
    "name": "cell28"
   },
   "outputs": [],
   "source": "describe table baseline_features_builing_dataset;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8562136d-46f6-4aed-9538-c6ad11d32588",
   "metadata": {
    "language": "python",
    "name": "label_dataset"
   },
   "outputs": [],
   "source": "# Now that we have created the dataset, we need to add the churn label\n    \ndef uc01_label_churn(session: snowpark.Session, baseline_table: str, output_table: str,  \n                     num_days_churn: int):\n\n    # Load baseline features dataset\n    baseline_df = session.table(baseline_table)\n\n    # Load sales dataset\n    sales_df = session.table(\"SALES\")\n\n    # Filter sales to retain only customer ID and transaction date\n    sales_filtered = sales_df.select(F.col(\"CUSTOMER_ID\"), F.col(\"TRANSACTION_DATE\"))\n\n    # Find the next transaction date for each (CUSTOMER_ID, TIMESTAMP)\n    next_transaction_df = (\n        baseline_df\n        .join(sales_filtered, \"CUSTOMER_ID\", \"left\")\n        .filter(F.col(\"TRANSACTION_DATE\") >F.col(\"LAST_PURCHASE_DATE\"))\n        .group_by(F.col(\"CUSTOMER_ID\"), F.col(\"TIMESTAMP\"))\n        .agg(F.min(\"TRANSACTION_DATE\").alias(\"NEXT_TRANSACTION_DATE\"))\n    )\n\n    # Join back with the baseline dataset to compute CHURNED\n    final_df = (\n        baseline_df\n        .join(next_transaction_df, [\"CUSTOMER_ID\", \"TIMESTAMP\"], \"left\")\n        .select(\n            baseline_df[\"*\"],\n            F.when(\n                (F.col(\"NEXT_TRANSACTION_DATE\").is_null()) |\n                ((F.col(\"NEXT_TRANSACTION_DATE\") - F.col(\"LAST_PURCHASE_DATE\")) > num_days_churn),\n                1\n            ).otherwise(0).alias(\"CHURNED\")\n        )\n    )\n\n    # Save the final labeled dataset\n    final_df.write.mode(\"overwrite\").save_as_table(output_table)\n\nsession.sproc.register(\n    func=uc01_label_churn,\n    name=\"uc_01_label_churn_sproc\",\n    replace=True,\n    is_permanent=True,\n    stage_location=\"@ML_STAGE\",\n    packages=['snowflake-snowpark-python', 'snowflake-ml-python'],\n    return_type=T.StringType()\n)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7e65d0c0-d806-4842-84f8-adb89c3b7b71",
   "metadata": {
    "language": "python",
    "name": "cell29"
   },
   "outputs": [],
   "source": "uc01_label_churn (session, 'baseline_features_builing_dataset', 'baseline_features_labeled', 30 )",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "82f58ed4-efc4-4edd-a387-1bca6ddf479b",
   "metadata": {
    "language": "sql",
    "name": "cell30"
   },
   "outputs": [],
   "source": "SELECT \n    TIMESTAMP,\n    SUM(CASE WHEN churned = 0 THEN 1 ELSE 0 END) AS not_churned,\n    SUM(CASE WHEN churned = 1 THEN 1 ELSE 0 END) AS churned\nFROM baseline_features_labeled\nGROUP BY TIMESTAMP\nORDER BY TIMESTAMP;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "581320c2-ca95-410e-9741-d6931abe1939",
   "metadata": {
    "language": "python",
    "name": "cell13"
   },
   "outputs": [],
   "source": "import snowflake.snowpark as snowpark\nimport  snowflake.snowpark.functions as F\nimport datetime\n\n\ndef unload_table_by_week(session: snowpark.Session, stage_name: str, table: str, date_column: str):\n    # Read sales data\n    df = session.table(table)\n\n    # Ensure transaction_date is in DATE format\n    #df = df.with_column(\"transaction_date\", to_date(col(\"transaction_date\")))\n\n    # Extract year and week for partitioning\n    df = df.with_column(\"year\", F.year(F.col(date_column)))\n    df = df.with_column(\"week\", F.weekofyear(F.col(date_column)))\n\n    # Get distinct year-week pairs\n    weeks = df.select(\"year\", \"week\").distinct().collect()\n\n    # Iterate over each week and export CSV\n    for row in weeks:\n        y, w = row[\"YEAR\"], row[\"WEEK\"]\n        output_file = f\"{table}_{y}_W{w}.csv\"\n        query = f\"\"\"\n            COPY INTO @{stage_name}/{output_file}\n            FROM (SELECT * FROM {table} WHERE year({date_column}) = {y} AND weekofyear({date_column}) = {w})\n            FILE_FORMAT = (TYPE = 'CSV' FIELD_OPTIONALLY_ENCLOSED_BY='\"')\n            SINGLE = TRUE;\n        \"\"\"\n        session.sql(query).collect()\n        print(f\"Exported {output_file} to {stage_name}\")\n\n    return f\"CSV export completed to {stage_name}\"",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d575144c-6aec-4ead-8919-070577b7636b",
   "metadata": {
    "language": "python",
    "name": "cell14"
   },
   "outputs": [],
   "source": "session.sql('use schema dev').collect()\nsession.sql(\"CREATE or replace stage CSV_STAGE\").collect()\nunload_table_by_week (session, 'CSV_STAGE', 'feedback_raw', 'chat_date')\nunload_table_by_week (session, 'CSV_STAGE', 'sales', 'transaction_date')\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "35607022-a3f3-406f-8474-9c58930c148a",
   "metadata": {
    "language": "sql",
    "name": "cell18"
   },
   "outputs": [],
   "source": "ls @CSV_STAGE;",
   "execution_count": null
  }
 ]
}